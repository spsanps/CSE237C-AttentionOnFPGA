{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of vector x.\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Assuming DMODEL, N, BITWIDTH, etc., are defined\n",
    "DMODEL = 16  # Example values, adjust as necessary\n",
    "N = 16       # Example values, adjust as necessary\n",
    "BITWIDTH = 4 # Bit-width for data_t\n",
    "BITWIDTH3 = 12 # Bit-width for data3_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_weights(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Regular expression to find matrices\n",
    "    matrices = re.findall(r\"{\\{(.*?)\\}\\}\", content, re.DOTALL)\n",
    "\n",
    "    make_int = lambda x: int(x.strip().strip(\"{\").strip(\"}\"))\n",
    "\n",
    "    # Process and store each matrix\n",
    "    Q_W = np.array(\n",
    "        [\n",
    "            list(map(make_int, row.strip(\",\").split(\",\")))\n",
    "            for row in matrices[0].split(\"\\n\")\n",
    "            if row.strip() != \"\"\n",
    "        ]\n",
    "    )\n",
    "    K_W = np.array(\n",
    "        [\n",
    "            list(map(make_int, row.strip(\",\").split(\",\")))\n",
    "            for row in matrices[1].split(\"\\n\")\n",
    "            if row.strip() != \"\"\n",
    "        ]\n",
    "    )\n",
    "    V_W = np.array(\n",
    "        [\n",
    "            list(map(make_int, row.strip(\",\").split(\",\")))\n",
    "            for row in matrices[2].split(\"\\n\")\n",
    "            if row.strip() != \"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return Q_W, K_W, V_W\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = f\"weights{DMODEL}.h\"  # Replace with the actual file path\n",
    "Q_W, K_W, V_W = read_weights(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]\n",
      " [13 13  7  3 12 13 15  6 15  8  8  1  0  9  2  8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming DMODEL, N, Q_W, K_W, V_W are defined\n",
    "\n",
    "# Step 1: Generate and normalize random tokens\n",
    "tokens = np.random.rand(N, DMODEL) * 15\n",
    "\n",
    "# Normalize Q, K, V matrices\n",
    "Q_W_norm = Q_W / np.linalg.norm(Q_W, axis=1, keepdims=True)\n",
    "K_W_norm = K_W / np.linalg.norm(K_W, axis=1, keepdims=True)\n",
    "V_W_norm = V_W / np.linalg.norm(V_W, axis=1, keepdims=True)\n",
    "\n",
    "# Step 2: Apply QKV transformations\n",
    "Q = tokens @ Q_W_norm\n",
    "K = tokens @ K_W_norm\n",
    "V = tokens @ V_W_norm\n",
    "\n",
    "# Step 3: Compute attention using softmax\n",
    "K_T = K.T\n",
    "attention_scores = Q @ K_T / np.sqrt(DMODEL)\n",
    "attention_weights = np.exp(attention_scores - np.max(attention_scores, axis=1, keepdims=True))\n",
    "attention_weights /= np.sum(attention_weights, axis=1, keepdims=True)\n",
    "attention_output = attention_weights @ V\n",
    "\n",
    "# Step 4: Normalize output tokens back into the range 0-15\n",
    "output_tokens = np.clip(np.round((attention_output - np.min(attention_output)) / (np.max(attention_output) - np.min(attention_output)) * 15), 0, 15).astype(int)\n",
    "\n",
    "# Output tokens\n",
    "print(output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_W: [[ 4 14 14 ... 11 13 12]\n",
      " [10 13 10 ... 13 14  7]\n",
      " [10 10  0 ... 11  8  9]\n",
      " ...\n",
      " [13 15  7 ... 14 13  4]\n",
      " [14 10  2 ...  8  4  6]\n",
      " [12 14  7 ...  4  2  5]]\n",
      "K_W: [[ 8  4  9 ...  6 12  8]\n",
      " [ 7 13  4 ...  6  5  4]\n",
      " [ 2  9  0 ... 15 11 13]\n",
      " ...\n",
      " [ 9  2  6 ...  4 15  5]\n",
      " [ 2 11 15 ...  0 11  2]\n",
      " [ 0  6  7 ...  4  8 13]]\n",
      "V_W: [[ 8 15 11 ...  9  7 14]\n",
      " [ 9 13 11 ...  0 15 15]\n",
      " [10  5  9 ...  6  7  9]\n",
      " ...\n",
      " [15 10  5 ...  9  5  4]\n",
      " [ 8 10  1 ...  4  8  8]\n",
      " [13  8 15 ...  2  5  4]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2176.47359756, 2217.60515701],\n",
       "        [2292.46851177, 2339.67972877]]),\n",
       " array([[3, 9],\n",
       "        [3, 9]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example values for demonstration\n",
    "DMODEL = 16  # Model dimension\n",
    "N = 16       # Number of tokens\n",
    "\n",
    "# Generate random weights for Q, K, V for demonstration purposes\n",
    "Q_W = np.random.rand(DMODEL, DMODEL)\n",
    "K_W = np.random.rand(DMODEL, DMODEL)\n",
    "V_W = np.random.rand(DMODEL, DMODEL)\n",
    "\n",
    "# Generate and normalize random tokens\n",
    "tokens = np.random.rand(N, DMODEL) * 15\n",
    "\n",
    "# Normalize Q, K, V matrices\n",
    "Q_W_norm = Q_W / np.linalg.norm(Q_W, axis=1, keepdims=True)\n",
    "K_W_norm = K_W / np.linalg.norm(K_W, axis=1, keepdims=True)\n",
    "V_W_norm = V_W / np.linalg.norm(V_W, axis=1, keepdims=True)\n",
    "\n",
    "# Apply QKV transformations\n",
    "Q = tokens @ Q_W_norm\n",
    "K = tokens @ K_W_norm\n",
    "V = tokens @ V_W_norm\n",
    "\n",
    "# Compute attention using softmax\n",
    "K_T = K.T\n",
    "attention_scores = Q @ K_T / np.sqrt(DMODEL)\n",
    "attention_weights = np.exp(attention_scores - np.max(attention_scores, axis=1, keepdims=True))\n",
    "attention_weights /= np.sum(attention_weights, axis=1, keepdims=True)\n",
    "attention_output = attention_weights @ V\n",
    "\n",
    "# Normalize output tokens back into the range 0-15\n",
    "output_tokens = np.clip(np.round((attention_output - np.min(attention_output)) / (np.max(attention_output) - np.min(attention_output)) * 15), 0, 15).astype(int)\n",
    "\n",
    "# Diagnostic prints\n",
    "attention_scores_diagnostic = attention_scores[:2, :2]  # Print a small part of the attention scores\n",
    "output_tokens_diagnostic = output_tokens[:2, :2]        # Print a small part of the output tokens\n",
    "\n",
    "attention_scores_diagnostic, output_tokens_diagnostic\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
